{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Recurrent Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import libraries.project_tests as tests\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, LSTM, Input, Dense, TimeDistributed, Embedding, Activation, RepeatVector, Bidirectional, Concatenate, Dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some problems the information required to make a prediction in one point of a sequence, includes not only pass information but also \"future\" information, i.e., information before and after of the target point in the sequence. This also implies that such informaction must be available to make the perdictions. For instance, in translation problems usually you need to know an entire sentence beforhand in order to translate it correctly.   The bidirectional RNNs are a modification of the standard RNNs that incorporate additional layers which transmit the information from the time $t+1$ to the time $t$. The forward and backward layers do not have any conextion among them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./Images/RNN_arc_3.png \"Neuronas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is based on the Machine Translation material included in the Deep Learning Specilization offered by Coursera: https://es.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model architecture could be used for a full language translation problem, however it would require hundred of thousands of texts, a big computational power (GPU) and hundreds of hours in order to get a fairly accurate model. Therefore, we are going to use a medium sizes datase that includes 137860 sentences in English and French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = tuple(open('data/small_vocab_en', 'r'))\n",
    "french_sentences = tuple(open('data/small_vocab_fr', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying padding to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 200\n",
      "French vocabulary size: 345\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the actual French translation, it is necessary to define a function to decode the network's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text_en(seq, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[item] for item in seq])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137860, 21, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_french_sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the output format and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 38s 346us/step - loss: 3.4585 - val_loss: 2.5706\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 38s 340us/step - loss: 2.4560 - val_loss: 2.3349\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 38s 341us/step - loss: 2.2076 - val_loss: 2.0730\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 38s 343us/step - loss: 1.9651 - val_loss: 1.8649\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 38s 344us/step - loss: 1.7908 - val_loss: 1.7219\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 38s 343us/step - loss: 1.6743 - val_loss: 1.6305\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 38s 342us/step - loss: 1.5973 - val_loss: 1.5651\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 38s 347us/step - loss: 1.5392 - val_loss: 1.5137\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 39s 354us/step - loss: 1.4924 - val_loss: 1.4710\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 38s 342us/step - loss: 1.4528 - val_loss: 1.4342\n",
      "Original sentence:\n",
      "new jersey is sometimes quiet during autumn and it is snowy in april <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Translated sentence:\n",
      "new jersey est parfois parfois en l' et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    input_seq = Input(input_shape[1:])\n",
    "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
    "    model = Model(input_seq, Activation('softmax')(logits))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                 optimizer = Adam(learning_rate))\n",
    "    \n",
    "    return model\n",
    "#Input and output sequences must have the same length\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print('Original sentence:')\n",
    "print(logits_to_text_en(tmp_x[:1].flatten(), english_tokenizer))\n",
    "print('Translated sentence:')\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 21, 1)             0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 21, 64)            12672     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 345)           22425     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 21, 345)           0         \n",
      "=================================================================\n",
      "Total params: 35,097\n",
      "Trainable params: 35,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 31s 277us/step - loss: 3.8471 - val_loss: 2.7391\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 28s 255us/step - loss: 2.4671 - val_loss: 2.1156\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 29s 264us/step - loss: 1.7596 - val_loss: 1.4836\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 29s 263us/step - loss: 1.3190 - val_loss: 1.1665\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 32s 289us/step - loss: 1.0585 - val_loss: 0.9588\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 30s 268us/step - loss: 0.8864 - val_loss: 0.8428\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 31s 280us/step - loss: 0.7931 - val_loss: 0.7416\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 30s 270us/step - loss: 0.7083 - val_loss: 0.6713\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 33s 302us/step - loss: 0.6513 - val_loss: 0.6293\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 31s 280us/step - loss: 0.6079 - val_loss: 0.5939\n",
      "Original sentence:\n",
      "new jersey is sometimes quiet during autumn and it is snowy in april <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Translated sentence:\n",
      "new jersey est parfois calme en l' et il est neigeux en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    rnn = GRU(64, return_sequences=True, activation=\"relu\")\n",
    "    \n",
    "    embedding = Embedding(english_vocab_size, 64, input_length=input_shape[1]) \n",
    "    logits = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"))\n",
    "    \n",
    "    model = Sequential()\n",
    "    #em can only be used in first layer --> Keras Documentation\n",
    "    model.add(embedding)\n",
    "    model.add(rnn)\n",
    "    model.add(logits)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate))\n",
    "    \n",
    "    return model\n",
    "#Input and output sequences must have the same length\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "embeded_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "embeded_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print('Original sentence:')\n",
    "print(logits_to_text_en(tmp_x[:1].flatten(), english_tokenizer))\n",
    "print('Translated sentence:')\n",
    "print(logits_to_text(embeded_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 44s 401us/step - loss: 3.4525 - val_loss: 2.5238\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 44s 396us/step - loss: 2.2618 - val_loss: 1.8411\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 42s 383us/step - loss: 1.5661 - val_loss: 1.3373\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 52s 472us/step - loss: 1.1882 - val_loss: 1.0314\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 52s 475us/step - loss: 0.9462 - val_loss: 0.8452\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 51s 465us/step - loss: 0.7941 - val_loss: 0.7139\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 52s 468us/step - loss: 0.6853 - val_loss: 0.6191\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 50s 452us/step - loss: 0.6029 - val_loss: 0.5483\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 48s 436us/step - loss: 0.5385 - val_loss: 0.4882\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 40s 362us/step - loss: 0.4866 - val_loss: 0.4413\n",
      "Original sentence:\n",
      "new jersey is sometimes quiet during autumn and it is snowy in april <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Translated sentence:\n",
      "new jersey est parfois calme pendant l'automne et il est de en en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "   \n",
    "    learning_rate = 1e-3\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 64, input_length=input_shape[1])) \n",
    "    model.add(Bidirectional(GRU(64, return_sequences = True, dropout = 0.1)))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
    "    model.compile(loss = sparse_categorical_crossentropy, \n",
    "                 optimizer = Adam(learning_rate))\n",
    "    return model\n",
    "\n",
    "#Input and output sequences must have the same length\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "bidi_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "bidi_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "# Print prediction(s)\n",
    "print('Original sentence:')\n",
    "print(logits_to_text_en(tmp_x[:1].flatten(), english_tokenizer))\n",
    "print('Translated sentence:')\n",
    "print(logits_to_text(bidi_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./Images/EDA.png \"Encoder-Decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image taken from: https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/20\n",
      "110288/110288 [==============================] - 173s 2ms/step - loss: 2.4363 - val_loss: 1.6915\n",
      "Epoch 2/20\n",
      "110288/110288 [==============================] - 175s 2ms/step - loss: 1.3726 - val_loss: 1.1235\n",
      "Epoch 3/20\n",
      "110288/110288 [==============================] - 177s 2ms/step - loss: 0.9688 - val_loss: 0.8357\n",
      "Epoch 4/20\n",
      "110288/110288 [==============================] - 167s 2ms/step - loss: 0.7145 - val_loss: 0.6113\n",
      "Epoch 5/20\n",
      "110288/110288 [==============================] - 160s 1ms/step - loss: 0.5060 - val_loss: 0.4110\n",
      "Epoch 6/20\n",
      "110288/110288 [==============================] - 160s 1ms/step - loss: 0.3338 - val_loss: 0.3191\n",
      "Epoch 7/20\n",
      "110288/110288 [==============================] - 161s 1ms/step - loss: 0.2621 - val_loss: 0.2205\n",
      "Epoch 8/20\n",
      "110288/110288 [==============================] - 155s 1ms/step - loss: 0.1778 - val_loss: 0.1699\n",
      "Epoch 9/20\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.1410 - val_loss: 0.1455\n",
      "Epoch 10/20\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.1238 - val_loss: 0.1499\n",
      "Epoch 11/20\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.1353 - val_loss: 0.1300\n",
      "Epoch 12/20\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.1023 - val_loss: 0.1011\n",
      "Epoch 13/20\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.0819 - val_loss: 0.0940\n",
      "Epoch 14/20\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.0777 - val_loss: 0.1173\n",
      "Epoch 15/20\n",
      "110288/110288 [==============================] - 150s 1ms/step - loss: 0.0852 - val_loss: 0.0924\n",
      "Epoch 16/20\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.0849 - val_loss: 0.0995\n",
      "Epoch 17/20\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.0755 - val_loss: 0.0824\n",
      "Epoch 18/20\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.0593 - val_loss: 0.0803\n",
      "Epoch 19/20\n",
      "110288/110288 [==============================] - 150s 1ms/step - loss: 0.0602 - val_loss: 0.0800\n",
      "Epoch 20/20\n",
      "110288/110288 [==============================] - 150s 1ms/step - loss: 0.0866 - val_loss: 0.1109\n",
      "Original sentence:\n",
      "new jersey is sometimes quiet during autumn and it is snowy in april <PAD> <PAD>\n",
      "Translated sentence:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "  \n",
    "    learning_rate = 0.005\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=english_vocab_size, output_dim=64, input_length=input_shape[1])) \n",
    "    model.add(Bidirectional(GRU(256, return_sequences = False)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(GRU(256, return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
    "    \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                 optimizer = Adam(learning_rate))\n",
    "    return model\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_english_sequence_length)\n",
    "#tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2]))\n",
    "\n",
    "encodeco_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "encodeco_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "\n",
    "print('Original sentence:')\n",
    "print(logits_to_text_en(tmp_x[:1].flatten(), english_tokenizer))\n",
    "print('Translated sentence:')\n",
    "print(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 15, 64)            12800     \n",
      "_________________________________________________________________\n",
      "bidirectional_20 (Bidirectio (None, 512)               493056    \n",
      "_________________________________________________________________\n",
      "repeat_vector_13 (RepeatVect (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "gru_42 (GRU)                 (None, 21, 256)           590592    \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 21, 345)           88665     \n",
      "=================================================================\n",
      "Total params: 1,185,113\n",
      "Trainable params: 1,185,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encodeco_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural machine translation with attention\n",
    "\n",
    "One of the problems of the previous model is the fact that the model has to memorize the entire sentence before start to translate it. The attention model introduces and additional layer that weight the contribution of the first bidirectional RNN layer's outputs to be feed into the last recurrent layer.\n",
    "\n",
    "\n",
    "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio , Neural Machine Translation by Jointly Learning to Align and Translate. https://arxiv.org/abs/1409.0473\n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"Images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"Images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following you can see the implementation of the attention model. Please pay attention to:\n",
    "\n",
    "**1) `one_step_attention()`**: At step $t$, given all the hidden states of the Bi-LSTM ($[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$) and the previous hidden state of the second LSTM ($s^{<t-1>}$), `one_step_attention()` will compute the attention weights ($[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$) and output the context vector (see Figure  1 (right) for details):\n",
    "$$context^{<t>} = \\sum_{t' = 0}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "Note that we are denoting the attention in this notebook $context^{\\langle t \\rangle}$. The coefficients must satisfy the constrain $\\sum_{t' = 0}^{T_x} \\alpha^{<t,t'>} = 1$.\n",
    "  \n",
    "**2) `model()`**: Implements the entire model. It first runs the input through a Bi-LSTM to get back $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$. Then, it calls `one_step_attention()` $T_y$ times (`for` loop). At each iteration of this loop, it gives the computed context vector $c^{<t>}$ to the second LSTM, and runs the output of the LSTM through a dense layer with softmax activation to generate a prediction $\\hat{y}^{<t>}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(max_english_sequence_length)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation('softmax', name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
    "    e = densor(concat)\n",
    "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(e)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 256\n",
    "n_s = 256\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(french_vocab_size, activation='softmax')\n",
    "\n",
    "def attention_model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx,))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    e = Embedding(human_vocab_size, 64)(X)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(e)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model([X, s0, c0], outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attention_model(max_english_sequence_length, max_french_sequence_length, n_a, n_s, english_vocab_size, french_vocab_size)\n",
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = preproc_french_sentences.shape[0]\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(preproc_french_sentences.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "137860/137860 [==============================] - 606s 4ms/step - loss: 47.7140 - dense_42_loss: 0.3419\n",
      "Epoch 2/20\n",
      "137860/137860 [==============================] - 390s 3ms/step - loss: 28.1594 - dense_42_loss: 0.0076\n",
      "Epoch 3/20\n",
      "137860/137860 [==============================] - 365s 3ms/step - loss: 21.6951 - dense_42_loss: 0.0035\n",
      "Epoch 4/20\n",
      "137860/137860 [==============================] - 374s 3ms/step - loss: 17.6323 - dense_42_loss: 0.0023\n",
      "Epoch 5/20\n",
      "137860/137860 [==============================] - 370s 3ms/step - loss: 15.1050 - dense_42_loss: 0.0019\n",
      "Epoch 6/20\n",
      "137860/137860 [==============================] - 365s 3ms/step - loss: 13.3470 - dense_42_loss: 0.0014\n",
      "Epoch 7/20\n",
      "137860/137860 [==============================] - 343s 2ms/step - loss: 11.9639 - dense_42_loss: 0.0011\n",
      "Epoch 8/20\n",
      "137860/137860 [==============================] - 341s 2ms/step - loss: 10.8752 - dense_42_loss: 9.0128e-04\n",
      "Epoch 9/20\n",
      "137860/137860 [==============================] - 347s 3ms/step - loss: 9.8615 - dense_42_loss: 7.7483e-04\n",
      "Epoch 10/20\n",
      "137860/137860 [==============================] - 377s 3ms/step - loss: 8.9909 - dense_42_loss: 6.8264e-04\n",
      "Epoch 11/20\n",
      "137860/137860 [==============================] - 370s 3ms/step - loss: 8.2156 - dense_42_loss: 6.0711e-04\n",
      "Epoch 12/20\n",
      "137860/137860 [==============================] - 398s 3ms/step - loss: 7.5953 - dense_42_loss: 5.8487e-04\n",
      "Epoch 13/20\n",
      "137860/137860 [==============================] - 381s 3ms/step - loss: 6.8813 - dense_42_loss: 5.2890e-04\n",
      "Epoch 14/20\n",
      "137860/137860 [==============================] - 373s 3ms/step - loss: 6.3664 - dense_42_loss: 4.8939e-04\n",
      "Epoch 15/20\n",
      "137860/137860 [==============================] - 363s 3ms/step - loss: 5.9118 - dense_42_loss: 4.4366e-04\n",
      "Epoch 16/20\n",
      "137860/137860 [==============================] - 360s 3ms/step - loss: 5.5162 - dense_42_loss: 4.1660e-04\n",
      "Epoch 17/20\n",
      "137860/137860 [==============================] - 362s 3ms/step - loss: 5.1944 - dense_42_loss: 3.7425e-04\n",
      "Epoch 18/20\n",
      "137860/137860 [==============================] - 363s 3ms/step - loss: 4.8760 - dense_42_loss: 3.5457e-04\n",
      "Epoch 19/20\n",
      "137860/137860 [==============================] - 362s 3ms/step - loss: 4.5770 - dense_42_loss: 3.4070e-04\n",
      "Epoch 20/20\n",
      "137860/137860 [==============================] - 373s 3ms/step - loss: 4.3552 - dense_42_loss: 3.1419e-04\n",
      "Original sentence:\n",
      "new jersey is sometimes quiet during autumn and it is snowy in april <PAD> <PAD>\n",
      "Translated sentence:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "model.fit([tmp_x, s0, c0], outputs, epochs=20, batch_size=1024)\n",
    "print('Original sentence:')\n",
    "print(logits_to_text_en(tmp_x[:1].flatten(), english_tokenizer))\n",
    "print('Translated sentence:')\n",
    "prediction = model.predict([tmp_x[:1], s0, c0])\n",
    "output = [logits_to_text(prediction[int(i)], french_tokenizer) for i in range(len(prediction))]\n",
    "print(' '.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_32 (Embedding)        (None, 15, 64)       12800       input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_29 (Bidirectional (None, 15, 512)      657408      embedding_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_19 (RepeatVector) (None, 15, 256)      0           s0[0][0]                         \n",
      "                                                                 lstm_10[0][0]                    \n",
      "                                                                 lstm_10[1][0]                    \n",
      "                                                                 lstm_10[2][0]                    \n",
      "                                                                 lstm_10[3][0]                    \n",
      "                                                                 lstm_10[4][0]                    \n",
      "                                                                 lstm_10[5][0]                    \n",
      "                                                                 lstm_10[6][0]                    \n",
      "                                                                 lstm_10[7][0]                    \n",
      "                                                                 lstm_10[8][0]                    \n",
      "                                                                 lstm_10[9][0]                    \n",
      "                                                                 lstm_10[10][0]                   \n",
      "                                                                 lstm_10[11][0]                   \n",
      "                                                                 lstm_10[12][0]                   \n",
      "                                                                 lstm_10[13][0]                   \n",
      "                                                                 lstm_10[14][0]                   \n",
      "                                                                 lstm_10[15][0]                   \n",
      "                                                                 lstm_10[16][0]                   \n",
      "                                                                 lstm_10[17][0]                   \n",
      "                                                                 lstm_10[18][0]                   \n",
      "                                                                 lstm_10[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 15, 768)      0           bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[0][0]           \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[1][0]           \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[2][0]           \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[3][0]           \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[4][0]           \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[5][0]           \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[6][0]           \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[7][0]           \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[8][0]           \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[9][0]           \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[10][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[11][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[12][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[13][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[14][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[15][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[16][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[17][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[18][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[19][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 repeat_vector_19[20][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 15, 1)        769         concatenate_5[0][0]              \n",
      "                                                                 concatenate_5[1][0]              \n",
      "                                                                 concatenate_5[2][0]              \n",
      "                                                                 concatenate_5[3][0]              \n",
      "                                                                 concatenate_5[4][0]              \n",
      "                                                                 concatenate_5[5][0]              \n",
      "                                                                 concatenate_5[6][0]              \n",
      "                                                                 concatenate_5[7][0]              \n",
      "                                                                 concatenate_5[8][0]              \n",
      "                                                                 concatenate_5[9][0]              \n",
      "                                                                 concatenate_5[10][0]             \n",
      "                                                                 concatenate_5[11][0]             \n",
      "                                                                 concatenate_5[12][0]             \n",
      "                                                                 concatenate_5[13][0]             \n",
      "                                                                 concatenate_5[14][0]             \n",
      "                                                                 concatenate_5[15][0]             \n",
      "                                                                 concatenate_5[16][0]             \n",
      "                                                                 concatenate_5[17][0]             \n",
      "                                                                 concatenate_5[18][0]             \n",
      "                                                                 concatenate_5[19][0]             \n",
      "                                                                 concatenate_5[20][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 15, 1)        0           dense_41[0][0]                   \n",
      "                                                                 dense_41[1][0]                   \n",
      "                                                                 dense_41[2][0]                   \n",
      "                                                                 dense_41[3][0]                   \n",
      "                                                                 dense_41[4][0]                   \n",
      "                                                                 dense_41[5][0]                   \n",
      "                                                                 dense_41[6][0]                   \n",
      "                                                                 dense_41[7][0]                   \n",
      "                                                                 dense_41[8][0]                   \n",
      "                                                                 dense_41[9][0]                   \n",
      "                                                                 dense_41[10][0]                  \n",
      "                                                                 dense_41[11][0]                  \n",
      "                                                                 dense_41[12][0]                  \n",
      "                                                                 dense_41[13][0]                  \n",
      "                                                                 dense_41[14][0]                  \n",
      "                                                                 dense_41[15][0]                  \n",
      "                                                                 dense_41[16][0]                  \n",
      "                                                                 dense_41[17][0]                  \n",
      "                                                                 dense_41[18][0]                  \n",
      "                                                                 dense_41[19][0]                  \n",
      "                                                                 dense_41[20][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1, 512)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[10][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[11][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[12][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[13][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[14][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[15][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[16][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[17][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[18][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[19][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "                                                                 attention_weights[20][0]         \n",
      "                                                                 bidirectional_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, 256), (None, 787456      dot_3[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_3[1][0]                      \n",
      "                                                                 lstm_10[0][0]                    \n",
      "                                                                 lstm_10[0][2]                    \n",
      "                                                                 dot_3[2][0]                      \n",
      "                                                                 lstm_10[1][0]                    \n",
      "                                                                 lstm_10[1][2]                    \n",
      "                                                                 dot_3[3][0]                      \n",
      "                                                                 lstm_10[2][0]                    \n",
      "                                                                 lstm_10[2][2]                    \n",
      "                                                                 dot_3[4][0]                      \n",
      "                                                                 lstm_10[3][0]                    \n",
      "                                                                 lstm_10[3][2]                    \n",
      "                                                                 dot_3[5][0]                      \n",
      "                                                                 lstm_10[4][0]                    \n",
      "                                                                 lstm_10[4][2]                    \n",
      "                                                                 dot_3[6][0]                      \n",
      "                                                                 lstm_10[5][0]                    \n",
      "                                                                 lstm_10[5][2]                    \n",
      "                                                                 dot_3[7][0]                      \n",
      "                                                                 lstm_10[6][0]                    \n",
      "                                                                 lstm_10[6][2]                    \n",
      "                                                                 dot_3[8][0]                      \n",
      "                                                                 lstm_10[7][0]                    \n",
      "                                                                 lstm_10[7][2]                    \n",
      "                                                                 dot_3[9][0]                      \n",
      "                                                                 lstm_10[8][0]                    \n",
      "                                                                 lstm_10[8][2]                    \n",
      "                                                                 dot_3[10][0]                     \n",
      "                                                                 lstm_10[9][0]                    \n",
      "                                                                 lstm_10[9][2]                    \n",
      "                                                                 dot_3[11][0]                     \n",
      "                                                                 lstm_10[10][0]                   \n",
      "                                                                 lstm_10[10][2]                   \n",
      "                                                                 dot_3[12][0]                     \n",
      "                                                                 lstm_10[11][0]                   \n",
      "                                                                 lstm_10[11][2]                   \n",
      "                                                                 dot_3[13][0]                     \n",
      "                                                                 lstm_10[12][0]                   \n",
      "                                                                 lstm_10[12][2]                   \n",
      "                                                                 dot_3[14][0]                     \n",
      "                                                                 lstm_10[13][0]                   \n",
      "                                                                 lstm_10[13][2]                   \n",
      "                                                                 dot_3[15][0]                     \n",
      "                                                                 lstm_10[14][0]                   \n",
      "                                                                 lstm_10[14][2]                   \n",
      "                                                                 dot_3[16][0]                     \n",
      "                                                                 lstm_10[15][0]                   \n",
      "                                                                 lstm_10[15][2]                   \n",
      "                                                                 dot_3[17][0]                     \n",
      "                                                                 lstm_10[16][0]                   \n",
      "                                                                 lstm_10[16][2]                   \n",
      "                                                                 dot_3[18][0]                     \n",
      "                                                                 lstm_10[17][0]                   \n",
      "                                                                 lstm_10[17][2]                   \n",
      "                                                                 dot_3[19][0]                     \n",
      "                                                                 lstm_10[18][0]                   \n",
      "                                                                 lstm_10[18][2]                   \n",
      "                                                                 dot_3[20][0]                     \n",
      "                                                                 lstm_10[19][0]                   \n",
      "                                                                 lstm_10[19][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 345)          88665       lstm_10[0][0]                    \n",
      "                                                                 lstm_10[1][0]                    \n",
      "                                                                 lstm_10[2][0]                    \n",
      "                                                                 lstm_10[3][0]                    \n",
      "                                                                 lstm_10[4][0]                    \n",
      "                                                                 lstm_10[5][0]                    \n",
      "                                                                 lstm_10[6][0]                    \n",
      "                                                                 lstm_10[7][0]                    \n",
      "                                                                 lstm_10[8][0]                    \n",
      "                                                                 lstm_10[9][0]                    \n",
      "                                                                 lstm_10[10][0]                   \n",
      "                                                                 lstm_10[11][0]                   \n",
      "                                                                 lstm_10[12][0]                   \n",
      "                                                                 lstm_10[13][0]                   \n",
      "                                                                 lstm_10[14][0]                   \n",
      "                                                                 lstm_10[15][0]                   \n",
      "                                                                 lstm_10[16][0]                   \n",
      "                                                                 lstm_10[17][0]                   \n",
      "                                                                 lstm_10[18][0]                   \n",
      "                                                                 lstm_10[19][0]                   \n",
      "                                                                 lstm_10[20][0]                   \n",
      "==================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 1,547,098\n",
      "Trainable params: 1,547,098\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics in a real context\n",
    "\n",
    "From: wikipedia\n",
    "\n",
    "**BLEU (bilingual evaluation understudy)** is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human: \"the closer a machine translation is to a professional human translation, the better it is\" – this is the central idea behind BLEU. BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics.\n",
    "\n",
    "Scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. Intelligibility or grammatical correctness are not taken into account\n",
    "\n",
    "NLTK provides the sentence_bleu() function for evaluating a candidate sentence against one or more reference sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
