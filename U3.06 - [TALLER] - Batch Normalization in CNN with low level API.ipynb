{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlutils\n",
    "reload(mlutils)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Convolutional network with TensorFlow low level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### RECOMMENDATION\n",
    "\n",
    "- close all applications\n",
    "- install Maxthon browser http://www.maxthon.com\n",
    "- open only VirtualBox and Maxthon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a small dataset based on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://s3.amazonaws.com/rlx/mini_cifar.h5\n",
    "import h5py\n",
    "with h5py.File('mini_cifar.h5','r') as h5f:\n",
    "    x_cifar = h5f[\"x\"][:]\n",
    "    y_cifar = h5f[\"y\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_cifar, y_cifar, test_size=.25)\n",
    "print x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "print \"\\ndistribution of train classes\"\n",
    "print pd.Series(y_train).value_counts()\n",
    "print \"\\ndistribution of test classes\"\n",
    "print pd.Series(y_test).value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Build a CNN with TF Low Level API\n",
    "\n",
    "### Build the convolutional network model\n",
    "\n",
    "with the same architecture as in the corresponding notebook:\n",
    "\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
    "    _________________________________________________________________\n",
    "    conv2d (Conv2D)              (None, 32, 32, 15)        735       \n",
    "    _________________________________________________________________\n",
    "    flatten (Flatten)            (None, 15360)             0         \n",
    "    _________________________________________________________________\n",
    "    dense (Dense)                (None, 16)                245776    \n",
    "    _________________________________________________________________\n",
    "    output_1 (Dense)             (None, 3)                 51        \n",
    "    =================================================================\n",
    "    Total params: 246,562\n",
    "    Trainable params: 246,562\n",
    "    Non-trainable params: 0\n",
    "    _________________________________________________________________\n",
    "\n",
    "#### understand carefully the example [here](http://www.jessicayung.com/explaining-tensorflow-code-for-a-convolutional-neural-network/). \n",
    "\n",
    "Complete the following function. You will have to:\n",
    "\n",
    "1. Declare tensor symbolic variables for inputs and model parameters:\n",
    "\n",
    "    - Define placefolders for X and y\n",
    "    - Define tf variables for W's and b's. You will have to think carefully about their shapes.\n",
    "\n",
    "\n",
    "2. Build the computational graph\n",
    "\n",
    "    - Use [tf.random_normal](https://www.tensorflow.org/api_docs/python/tf/random/normal) with mean 0 and std 1 as initialization distribuition for all W's and b's\n",
    "    - Use [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) for the convolutional layer (`h_conv1`)\n",
    "    - Use [tf.reshape](https://www.tensorflow.org/api_docs/python/tf/reshape) to transition from the convolutional layer to the dense layer (`h_conv1_flat`)\n",
    "    - Model the dense layer with TF matrix multiplication and relu activation (`h_dense`)\n",
    "    - Model the output with three output neurons and softmax activation (`y_proba`)\n",
    "\n",
    "the shapes of the weights your define must be equal to the ones printed out in the corresponding notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_filters, filter_size, dense_size, img_size, n_channels):\n",
    "\n",
    "    init_stddev = 0.01\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    tX = ...\n",
    "    ty = ...\n",
    "\n",
    "    w_conv1 = ...\n",
    "    b_conv1 = ...\n",
    "\n",
    "    w_dense = ...\n",
    "    b_dense = ...\n",
    "\n",
    "    w_out   = ...\n",
    "    b_out   = ...\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"cnn\"):\n",
    "        h_conv1      = ...\n",
    "        h_conv1_flat = ...\n",
    "        h_dense      = ...\n",
    "        y_proba      = ...\n",
    "\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        y_hat        = tf.argmax(y_proba, axis=1)\n",
    "        xentropy     = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_proba, labels=ty)\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        loss         = tf.reduce_mean(xentropy)\n",
    "        optimizer    = tf.train.AdamOptimizer()\n",
    "        training_op  = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(y_proba,ty,1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    print \"-- weights shape --\"\n",
    "    print w_conv1.shape\n",
    "    print b_conv1.shape\n",
    "    print w_dense.shape\n",
    "    print b_dense.shape\n",
    "    print w_out.shape\n",
    "    print b_out.shape \n",
    "\n",
    "    return tX, ty, init, accuracy, training_op, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y_cifar))\n",
    "print \"using\", n_classes, \"classes\"\n",
    "\n",
    "n_filters   = 15\n",
    "filter_size = 4\n",
    "dense_size  = 16\n",
    "n_channels  = 3 \n",
    "img_size    = 32\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, ty, init, accuracy, training_op, loss = build_model(n_filters, filter_size, dense_size, img_size, n_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the optimization loop\n",
    "\n",
    "keep track of accuracy and loss in both train and test. Base your implementation on the notebook describing TF low level API.\n",
    "\n",
    "Observe that accuracy must keep one metric per epoch averaging the accuracy obtained in all batches. Likewise for loss.\n",
    "\n",
    "Plot the accuracy and loss curves for test and train separately, which should look like the following\n",
    "\n",
    "![AA](Images/lab_batch_01.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_examples = len(x_train)\n",
    "acc_train, acc_test = [], []\n",
    "loss_train, loss_test = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        idxs = np.random.permutation(len(x_train))\n",
    "        eacc, eloss = [], []\n",
    "        \n",
    "        for iteration in range(num_examples // batch_size):\n",
    "            \n",
    "            X_batch = ...\n",
    "            y_batch = ...\n",
    "            _, _eacc, _eloss = ...\n",
    "            \n",
    "            eacc += [_eacc]\n",
    "            eloss += [_eloss]\n",
    "            \n",
    "        acc_train += ...\n",
    "        acc_test  += ...\n",
    "        loss_train += ...\n",
    "        loss_test  += ...\n",
    "        \n",
    "        print \"epoch: %3d\"%(epoch+1), \"  train accuracy: %.4f\"%acc_train[-1], \"  test accuracy: %.4f\"%acc_test[-1], \"  train loss: %.4f\"%loss_train[-1], \"  test loss: %.4f\"%loss_test[-1]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ... plot accuracy and loss for train and test ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "Modify the optimization loop so that each batch is normalized before feeding it to the optimization step according to the following spec:\n",
    "\n",
    "- consider only $X^{i}$ in the current batch\n",
    "- $X^{i}$: image $i$\n",
    "- $X^{i}_{j|k}$: channel $k$ of pixel $j$ in image $i$\n",
    "- $S^i$: image $i$ standardized\n",
    "\n",
    "In pixel wise standardization, each pixel has zero mean and std=1 across the dataset:\n",
    "\n",
    "- $\\mu = \\frac{1}{N}\\sum_{i,j,k} X^{i}_{j|k}$\n",
    "- $\\sigma = \\frac{1}{N}\\sum_{i,j,k}^{N-1}(X^{i}_{j|k}-\\mu_{j|k})^2$\n",
    "\n",
    "So that:\n",
    "\n",
    "$$S^{i}_{j|k} = \\frac{1}{\\sigma + 10^{-6}}(X^{i}_{j|k} - \\mu)$$\n",
    "\n",
    "\n",
    "The $10^{-6}$ is to avoid the case of zero variance\n",
    "\n",
    "you must also plot:\n",
    "\n",
    "- accuracy and loss curves for train and test separately, which should look better than the previous\n",
    "\n",
    "![](Images/lab_batch_02.png)\n",
    "\n",
    "- for only train, the accuracy and loss curves of both experiments, looking like this\n",
    "\n",
    "![](Images/lab_batch_03.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, ty, init, accuracy, training_op, loss = build_model(n_filters, filter_size, dense_size, img_size, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx_test = np.r_[[(i-np.mean(i))/np.std(i) for i in x_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(x_train)\n",
    "bacc_train, bacc_test = [], []\n",
    "bloss_train, bloss_test = [], []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        ...\n",
    "        for iteration in range(num_examples // batch_size):\n",
    "            ...\n",
    "            \n",
    "        ...\n",
    "        print \"epoch: %3d\"%(epoch+1), \"  train accuracy: %.4f\"%bacc_train[-1], \"  test accuracy: %.4f\"%bacc_test[-1], \"  train loss: %.4f\"%bloss_train[-1], \"  test loss: %.4f\"%bloss_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ... plot accuracy and loss for train in both experiments ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p27cpu",
   "language": "python",
   "name": "p27cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
